<!doctype html>
<meta charset="utf-8">
<script src="assets/d3.min.js"></script>
<script src="http://distill.pub/template.v1.js"></script>
<script type="text/front-matter">
  title: Attention and Augmented Recurrent Neural Networks
  description: A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.
  authors:
    - Chris Olah: http://colah.github.io
    - Shan Carter: http://shancarter.com
  affiliations:
    - Google Brain: http://g.co/brain
    - Google Brain: http://g.co/brain
</script>

<style>
#previews a {
  text-decoration: none;
  overflow: hidden;
  margin-bottom: 12px;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
  padding-bottom: 12px;

}
#previews figcaption {
  margin-left: 100px;
}
#previews svg, #previews img {
  width: 80px;
  float: left;
}

@media(min-width: 768px) {
  #previews {
    overflow: hidden;
    margin-top: 48px;
    margin-bottom: 48px;
  }
  #previews figcaption {
    margin-left: 0;
  }
  #previews figcaption b {
    display: block;
  }
  #previews figcaption b span {
    display: block;
  }
  #previews a {
    position: relative;
    float: left;
    width: 19%;
    margin-right: 3.6%;
    padding-right: 3.6%;
    border-right: 1px solid rgba(0, 0, 0, 0.05);
    border-bottom: none;
    margin-bottom: 0;
    padding-bottom: 0;
  }
  #previews a:last-child {
    margin-right: 0;
    padding-right: 0;
    border-right: 0;
  }
  #previews svg, #previews img {
    margin-bottom: 18px;
    display: block;
    width: 100%;
    float: none;
  }
}

@media(min-width: 1024px) {
  #previews {
    margin-top: 90px;
    margin-bottom: 90px;
  }
}
</style>

<dt-article class="centered">
  <h1>Attention and Augmented Recurrent Neural Networks</h1>

  <dt-byline></dt-byline>

  <p>Recurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of data like text, audio and video. They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch!</p>

  <p><figure class="l-body">
    <dt-include src="assets/rnn_basic_rnn.svg"></dt-include>
  </figure></p>

  <p>The basic RNN design struggles with longer sequences, but a special variant--<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">"long short-term memory" networks</a> <dt-cite key="olah2015understanding"></dt-cite>--can even work with these. Such models have been found to be very powerful, achieving remarkable results in many tasks including translation, voice recognition, and image captioning. As a result, recurrent neural networks have become very widespread in the last few years.</p>

  <p>As this has happened, we’ve seen a growing number of attempts to augment RNNs with new properties. Four directions stand out as particularly exciting:</p>

  <figure class="l-middle" id="previews">
    <a href="#neural-turing-machines">
      <img src="assets/rnn_preview_ntm.svg">
      <figcaption><b><span>Neural Turing</span> Machines</b> have external memory that they can read and write to.</figcaption>
    </a>
    <a href="#attentional-interfaces">
      <img src="assets/rnn_preview_ai.svg">
      <figcaption><b><span>Attentional</span> Interfaces</b> allow RNNs to focus on parts of their input.</figcaption>
    </a>
    <a href="#adaptive-computation-time">
      <img src="assets/rnn_preview_act.svg">
      <figcaption><b><span>Adaptive</span> Computation Time</b> allows for varying amounts of computation per step.</figcaption>
    </a>
    <a href="#neural-programmer">
      <img src="assets/rnn_preview_np.svg">
      <figcaption><b><span>Neural</span> Programmers</b> can call functions, building programs as they run.</figcaption>
    </a>
  </figure>

  <style>
    #previews a {
      text-decoration: none;
      overflow: hidden;
      margin-bottom: 12px;
      display: block;
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
      margin-bottom: 12px;
      padding-bottom: 12px;

  }
    #previews figcaption {
      margin-left: 100px;
    }
    #previews svg, #previews img {
      width: 80px;
      float: left;
    }

  @media(min-width: 768px) {
      #previews {
        overflow: hidden;
        margin-top: 48px;
        margin-bottom: 48px;
      }
      #previews figcaption {
        margin-left: 0;
      }
      #previews figcaption b {
        display: block;
      }
      #previews figcaption b span {
        display: block;
      }
      #previews a {
        position: relative;
        float: left;
        width: 19%;
        margin-right: 3.6%;
        padding-right: 3.6%;
        border-right: 1px solid rgba(0, 0, 0, 0.05);
        border-bottom: none;
        margin-bottom: 0;
        padding-bottom: 0;
      }
      #previews a:last-child {
        margin-right: 0;
        padding-right: 0;
        border-right: 0;
      }
      #previews svg, #previews img {
        margin-bottom: 18px;
        display: block;
        width: 100%;
        float: none;
      }
    }

  @media(min-width: 1024px) {
    #previews {
      margin-top: 90px;
      margin-bottom: 90px;
    }
  }
  </style>

  <p>Individually, these techniques are all potent extensions of RNNs, but the really striking thing is that they can be combined, and seem to just be points in a broader space. Further, they all rely on the same underlying trick--something called attention--to work.</p>

  <p>Our guess is that these "augmented RNNs" will have an important role to play in extending deep learning's capabilities over the coming years.</p>

  <hr />

  <h2 id="neural-turing-machines">Neural Turing Machines</h2>

  <p>Neural Turing Machines <dt-cite key="graves2014neural"></dt-cite> combine a RNN with an external memory bank. Since vectors are the natural language of neural networks, the memory is an array of vectors:</p>

  <p><figure class="l-body-outset"  id="rnn-memory">
    <dt-include src="assets/rnn_memory.svg"></dt-include>
  </figure></p>

  <p>But how does reading and writing work? The challenge is that we want to make them differentiable. In particular, we want to make them differentiable with respect to the location we read from or write to, so that we can learn where to read and write. This is tricky because memory addresses seem to be fundamentally discrete. NTMs take a very clever solution to this: every step, they read and write everywhere, just to different extents.</p>

  <p>As an example, let’s focus on reading. Instead of specifying a single location, the RNN outputs an “attention distribution” that describes how we spread out the amount we care about different memory positions. As such, the result of the read operation is a weighted sum.</p>

  <p><figure class="l-body-outset" id="rnn-read">
    <dt-include src="assets/rnn_read.svg"></dt-include>
  </figure></p>

  <p>Similarly, we write everywhere at once to different extents. Again, an attention distribution describes how much we write at every location. We do this by having the new value of a position in memory be a convex combination of the old memory content and the write value, with the position between the two decided by the attention weight.</p>

  <p><figure class="l-body-outset" id="rnn-write">
    <dt-include src="assets/rnn_write.svg"></dt-include>
  </figure></p>

  <p><dt-include src="assets/rnn_write.html"></dt-include></p>

  <p>But how do NTMs decide which positions in memory to focus their attention on? They actually use a combination of two different methods: content-based attention and location-based attention. Content-based attention allows NTMs to search through their memory and focus on places that match what they’re looking for, while location-based attention allows relative movement in memory, enabling the NTM to loop.</p>

  <p><figure class="l-page" id="rnn-write-detail">
    <dt-include src="assets/rnn_write_detail.svg"></dt-include>
  </figure></p>

  <p><dt-include src="assets/rnn_write_detail.html"></dt-include></p>

  <p>This capability to read and write allows NTMs to perform many simple algorithms, previously beyond neural networks. For example, they can learn to store a long sequence in memory, and then loop over it, repeating it back repeatedly. As they do this, we can watch where they read and write, to better understand what they're doing:</p>

  <p><figure class="l-body side">
    <img src="assets/NTM-Copy-ReadWrite.svg"></img>
    <figcaption style="bottom: 0px;">See more experiments in <dt-cite key="xu2015show"></dt-cite>. This figure is based on the Repeat Copy experiment.</figcaption>
  </figure></p>

  <p>They can also learn to mimic a lookup table, or even learn to sort numbers (although they kind of cheat)! On the other hand, they still can’t do many basic things, like add or multiply numbers.</p>

  <p>Since the original NTM paper, there have been a number of exciting papers exploring similar directions. The Neural GPU <dt-cite key="kaiser2015neural"></dt-cite> overcomes the NTM's inability to add and multiply numbers.  <dt-cite key="zaremba2015reinforcement">Zaremba & Sutskever</dt-cite> train NTMs using reinforcement learning instead of the differentiable read/writes used by the original. Neural Random Access Machines <dt-cite key="kurach2015neural"></dt-cite> work based on pointers. Some papers have explored differentiable data structures, like stacks and queues <dt-cite key="grefenstette2015learning,joulin2015inferring"></dt-cite>. And memory networks <dt-cite key="weston2014memory,kumar2015ask"></dt-cite> are another approach to attacking similar problems.</p>

  <p>In some objective sense, many of the tasks these models can perform--such as learning how to add numbers--aren't that objectively hard. The traditional program synthesis community would eat them for lunch. But neural networks are capable of many other things, and models like the Neural Turing Machine seem to have knocked away a very profound limit on their abilities.</p>

  <h3>Code</h3>

  <p>There are a number of open source implementations of these models. Open source implementations of the Neural Turing Machine include <a href="https://github.com/carpedm20/NTM-tensorflow">Taehoon Kim's</a> (TensorFlow), <a href="https://github.com/shawntan/neural-turing-machines">Shawn Tan's</a> (Theano), <a href="https://github.com/fumin/ntm">Fumin's</a> (Go), <a href="https://github.com/kaishengtai/torch-ntm">Kai Sheng Tai's</a> (Torch), and <a href="https://github.com/snipsco/ntm-lasagne">Snip's</a> (Lasagne). Code for the Neural GPU publication was open sourced and put in the <a href="https://github.com/tensorflow/models/tree/master/research/neural_gpu">TensorFlow Models repository</a>. Open source implementations of Memory Networks include <a href="https://github.com/facebook/MemNN">Facebook's</a> (Torch/Matlab), <a href="https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano">YerevaNN's</a> (Theano), and <a href="https://github.com/carpedm20/MemN2N-tensorflow">Taehoon Kim's</a> (TensorFlow). </span></p></p>

  <hr />

  <h2 id="attentional-interfaces">Attentional Interfaces</h2>

  <p>When I’m translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so.</p>

  <p>Neural networks can achieve this same behavior using <em>attention</em>, focusing on part of a subset of the information they're given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.</p>

  <p>We'd like attention to be differentiable, so that we can learn where to focus. To do this, we use the same trick Neural Turing Machines use: we focus everywhere, just to different extents.</p>

  <p><figure class="l-body">
    <dt-include src="assets/rnn_attentional_01.svg"></dt-include>
  </figure></p>

  <p>The attention distribution is usually generated with content-based attention. The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution.</p>

  <p><figure class="l-body">
    <dt-include src="assets/rnn_attentional_02.svg"></dt-include>
  </figure></p>

  <p>One use of attention between RNNs is translation <dt-cite key="bahdanau2014neural"></dt-cite>. A traditional sequence-to-sequence model has to boil the entire input down into a single vector and then expands it back out. Attention avoids this by allowing the RNN processing the input to pass along information about each word it sees, and then for the RNN generating the output to focus on words as they become relevant.</p>

  <p><dt-include src="assets/rnn_attentional_ex2.html"></dt-include></p>

  <p>This kind of attention between RNNs has a number of other applications. It can be used in voice recognition <dt-cite key="chan2015listen"></dt-cite>, allowing one RNN to process the audio and then have another RNN skim over it, focusing on relevant parts as it generates a transcript.</p>

  <p><dt-include src="assets/rnn_attentional_ex3.html"></dt-include></p>

  <p>Other uses of this kind of attention include parsing text <dt-cite key="vinyals2015grammar"></dt-cite>, where it allows the model to glance at words as it generates the parse tree, and for conversational modeling <dt-cite key="vinyals2015neural"></dt-cite>, where it lets the model focus on previous parts of the conversation as it generates its response.</p>

  <p>Attention can also be used on the interface between a convolutional neural network and an RNN. This allows the RNN to look at different position of an image every step. One popular use of this kind of attention is for image captioning. First, a conv net processes the image, extracting high-level features. Then an RNN runs, generating a description of the image. As it generates each word in the description, the RNN focuses on the conv net's interpretation of the relevant parts of the image. We can explicitly visualize this:</p>

  <p><figure class="l-body-outset external">
    <img src="assets/show-attend-tell.png">
    <figcaption style="bottom: 0px;">Figure from <dt-cite key="xu2015show"></dt-cite></figcaption>
  </figure></p>

  <p>More broadly, attentional interfaces can be used whenever one wants to interface with a neural network that has a repeating structure in its output.</p>

  <p>Attentional interfaces have been found to be an extremely general and powerful technique, and are becoming increasingly widespread.</p>

  <hr />

  <h2 id="adaptive-computation-time">Adaptive Computation Time</h2>

  <p class="equation-mimic">Standard RNNs do the same amount of computation for each time step. This seems unintuitive. Surely, one should think more when things are hard? It also limits RNNs to doing <span class="equation-mimic">O(n)</span> operations for a list of length <span class="equation-mimic">n</span>.</p>

  <p>Adaptive Computation Time <dt-cite key="graves2016adaptive"></dt-cite> is a way for RNNs to do different amounts of computation each step. The big picture idea is simple: allow the RNN to do multiple steps of computation for each time step.</p>

  <p>In order for the network to learn how many steps to do, we want the number of steps to be differentiable. We achieve this with the same trick we used before: instead of deciding to run for a discrete number of steps, we have an attention distribution over the number of steps to run. The output is a weighted combination of the outputs of each step.</p>

  <p><figure class="l-page">
    <dt-include src="assets/rnn_adaptive_01.svg"></dt-include>
  </figure></p>

  <p>There are a few more details, which were left out in the previous diagram. Here's a complete diagram of a time step with three computation steps.</p>

  <p><figure class="l-body-outset">
    <dt-include src="assets/rnn_adaptive_02.svg"></dt-include>
  </figure></p>

  <p>That's a bit complicated, so let's work through it step by step. At a high-level, we're still running the RNN and outputting a weighted combination of the states:</p>

  <p><figure class="l-body-outset">
    <dt-include src="assets/rnn_adaptive_02_1.svg"></dt-include>
  </figure></p>

  <p>The weight for each step is determined by a "halting neuron." It's a sigmoid neuron that looks at the RNN state and gives a halting weight, which we can think of as the probability that we should stop at that step.</p>

  <p><figure class="l-body-outset">
    <dt-include src="assets/rnn_adaptive_02_2.svg"></dt-include>
  </figure></p>

  <p>We have a total budget for the halting weights of 1, so we track that budget along the top. When it gets to less than epsilon, we stop.</p>

  <p><figure class="l-body-outset">
    <dt-include src="assets/rnn_adaptive_02_3.svg"></dt-include>
  </figure></p>

  <p>When we stop, might have some left over halting budget because we stop when it gets to less than epsilon. What should we do with it? Technically, it's being given to future steps but we don't want to compute those, so we attribute it to the last step.</p>

  <p><figure class="l-body-outset">
    <dt-include src="assets/rnn_adaptive_02_4.svg"></dt-include>
  </figure></p>

  <p>When training Adaptive Computation Time models, one adds a "ponder cost" term to the cost function. This penalizes the model for the amount of computation it uses. The bigger you make this term, the more it will trade-off performance for lowering compute time.</p>

  <p>Adaptive Computation Time is a very new idea, but we believe that it, along with similar ideas, will be very important.</p>

  <h3>Code</h3>

  <p>The only open source implementation of Adaptive Computation Time at the moment seems to be <a href="https://github.com/DeNeutoy/act-tensorflow">Mark Neumann's</a> (TensorFlow).</p>

  <hr />

  <h2 id="neural-programmer">Neural Programmer</h2>

  <p>Neural nets are excellent at many tasks, but they also struggle to do some basic things like arithmetic, which are trivial in normal approaches to computing. It would be really nice to have a way to fuse neural nets with normal programming, and get the best of both worlds.</p>

  <p>The neural programmer <dt-cite key="neelakantan2015neural"></dt-cite> is one approach to this. It learns to create programs in order to solve a task. In fact, it learns to generate such programs <em>without needing examples of correct programs</em>. It discovers how to produce programs as a means to the end of accomplishing some task.</p>

  <p>The actual model in the paper answers questions about tables by generating SQL-like programs to query the table. However, there are a number of details here that make it a bit complicated, so let's start by imagining a slightly simpler model, which is given an arithmetic expression and generates a program to evaluate it.</p>

  <p>The generated program is a sequence of operations. Each operation is defined to operate on the output of past operations. So an operation might be something like "add the output of the operation 2 steps ago and the output of the operation 1 step ago." It's more like a Unix pipe than a program with variables being assigned to and read from.</p>

  <p><figure class="l-body">
    <dt-include src="assets/rnn_programmer_1.svg"></dt-include>
  </figure></p>

  <p>The program is generated one operation at a time by a controller RNN. At each step, the controller RNN outputs a probability distribution for what the next operation should be. For example, we might be pretty sure we want to perform addition at the first time step, then have a hard time deciding whether we should multiply or divide at the second step, and so on...</p>

  <p><figure class="l-body">
    <dt-include src="assets/rnn_programmer_2.svg"></dt-include>
  </figure></p>

  <p>The resulting distribution over operations can now be evaluated. Instead of running a single operation at each step, we do the usual attention trick of running all of them and then average the outputs together, weighted by the probability we ran that operation.</p>

  <p><figure class="l-body">
    <dt-include src="assets/rnn_programmer_3.svg"></dt-include>
  </figure></p>

  <p>As long as we can define derivatives through the operations, the program's output is differentiable with respect to the probabilities. We can then define a loss, and train the neural net to produce programs that give the correct answer. In this way, the Neural Programmer learns to produce programs without examples of good programs. The only supervision is the answer the program should produce.</p>

  <p>That's the core idea of Neural Programmer, but the version in the paper answers questions about tables, rather than arithmetic expressions. There are a few additional neat tricks:</p>

  <ul>
  <li><p><strong>Multiple Types:</strong> Many of the operations in the Neural Programmer deal with types other than scalar numbers. Some operations output selections of table columns or selections of cells. <!-- footnote? (To allow us to backprop through the selecting things and average selections, we allow things to be selected to different extents, with 0 as unselected and 1 as fully selected.) --> Only outputs of the same type get merged together.</p></li>
  <li><p><strong>Referencing Inputs:</strong> The neural programmer needs to answer questions like "How many cities have a population greater than 1,000,000?" given a table of cities with a population column. To facilitate this, some operations allow the network to reference constants in the question they're answering, or the names of columns. This referencing happens by attention, in the style of pointer networks <dt-cite key="vinyals2015pointer"></dt-cite>. <!-- For example, in order to use the *Greater* operation, the controller must select a value that table entries are greater than; instead of using a previous scalar value it's computed, it has the controller select a value in the question using attention. --></p></li>
  </ul>

  <p>The Neural Programmer isn't the only approach to having neural networks generate programs. Another lovely approach is the Neural Programmer-Interpreter <dt-cite key="reed2015neural"></dt-cite> which can accomplish a number of very interesting tasks, but requires supervision in the form of correct programs.</p>

  <p>We think that this general space, of bridging the gap between more traditional programming and neural networks is extremely important. While the Neural Programmer is clearly not the final solution, we think there are a lot of important lessons to be learned from it.</p>

  <h3>Code</h3>

  <p>The more recent version of Neural Programmer for <a href="https://openreview.net/pdf?id=ry2YOrcge">question answering</a> has been open sourced by its authors and is available as a <a href="https://github.com/tensorflow/models/tree/master/research/neural_programmer">TensorFlow Model</a>. There is also an implementation of the Neural Programmer-Interpreter by <a href="https://github.com/mokemokechicken/keras_npi">Ken Morishita</a> (Keras).</p>

  <hr />

  <h2>The Big Picture</h2>

  <p>A human with a piece of paper is, in some sense, much smarter than a human without. A human with mathematical notation can solve problems they otherwise couldn't. Access to computers makes us capable of incredible feats that would otherwise be far beyond us.</p>

  <p>In general, it seems like a lot of interesting forms of intelligence are an interaction between the creative heuristic intuition of humans and some more crisp and careful media, like language or equations. Sometimes, the medium is something that physically exists, and stores information for us, prevents us from making mistakes, or does computational heavy lifting. In other cases, the medium is a model in our head that we manipulate. Either way, it seems deeply fundamental to intelligence.</p>

  <p>Recent results in machine learning have started to have this flavor, combining the intuition of neural networks with something else. One approach is what one might call "heuristic search." For example, AlphaGo <dt-cite key="silver2016mastering"></dt-cite> has a model of how Go works and explores how the game could play out guided by neural network intuition. Similarly, DeepMath <dt-cite key="alemi2016deepmath"></dt-cite> uses neural networks as intuition for manipulating mathematical expressions. The "augmented RNNs" we've talked about in this article are another approach, where we connect RNNs to engineered media, in order to extend their general capabilities.</p>

  <p>Interacting with media naturally involves making a sequence of taking an action, observing, and taking more actions. This creates a major challenge: how do we learn which actions to take? That sounds like a reinforcement learning problem and we could certainly take that approach. But the reinforcement learning literature is really attacking the hardest version of this problem, and its solutions are hard to use. The wonderful thing about attention is that it gives us an easier way out of this problem by partially taking all actions to varying extents. This works because we can design media--like the NTM memory--to allow fractional actions and to be differentiable. Reinforcement learning has us take a single path, and try to learn from that. Attention takes every direction at a fork, and then merges the paths back together.</p>

  <p>A major weaknesses of attention is that we have to take every "action" every step. This causes the computational cost to grow linearly as you do things like increase the amount of memory in a Neural Turing Machine. One thing you could imagine doing is having your attention be sparse, so that you only have to touch some memories. However, it's still challenging because you may want to do things like have your attention depend on the content of the memory, and doing that naively forces you to look at each memory. We've seen some initial attempts to attack this problem, such as <dt-cite key="andrychowicz2016learning"></dt-cite>, but it seems like there's a lot more to be done. If we could really make such sub-linear time attention work, that would be very powerful!</p>

  <p>Augmented recurrent neural networks, and the underlying technique of attention, are incredibly exciting. We look forward to seeing what happens next!</p>


</dt-article>

<!-- Appendix -->
<dt-appendix class="centered">
  <h3>Acknowledgments</h3>
  <p>Thank you to Maithra Raghu, Dario Amodei, Cassandra Xia, Luke Vilnis, Anna Goldie, Jesse Engel, Dan Mané, Natasha Jaques, Emma Pierson and Ian Goodfellow for their feedback and encouragement. We're also very grateful to our team, <a href="http://g.co/brain">Google Brain</a>, for being extremely supportive of our project.</p>

</dt-appendix>
<dt-footer></dt-footer>
<script type="text/bibliography">
@article{alemi2016deepmath,
  author    = {Alex A. Alemi and
               Fran{\c{c}}ois Chollet and
               Geoffrey Irving and
               Christian Szegedy and
               Josef Urban},
  title     = {DeepMath - Deep Sequence Models for Premise Selection},
  journal   = {CoRR},
  volume    = {abs/1606.04442},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.04442},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/AlemiCISU16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{andrychowicz2016learning,
  author    = {Marcin Andrychowicz and
               Karol Kurach},
  title     = {Learning Efficient Algorithms with Hierarchical Attentive Memory},
  journal   = {CoRR},
  volume    = {abs/1602.03218},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.03218},
  timestamp = {Tue, 01 Mar 2016 17:47:25 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/AndrychowiczK16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{chan2015listen,
  author    = {William Chan and
               Navdeep Jaitly and
               Quoc V. Le and
               Oriol Vinyals},
  title     = {Listen, Attend and Spell},
  journal   = {CoRR},
  volume    = {abs/1508.01211},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.01211},
  timestamp = {Tue, 01 Sep 2015 14:42:40 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChanJLV15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{dzmitry2014neural,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 01 Oct 2014 15:00:04 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{graves2014neural,
  author    = {Alex Graves and
               Greg Wayne and
               Ivo Danihelka},
  title     = {Neural Turing Machines},
  journal   = {CoRR},
  volume    = {abs/1410.5401},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.5401},
  timestamp = {Sun, 02 Nov 2014 11:25:59 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{graves2016adaptive,
  author    = {Alex Graves},
  title     = {Adaptive Computation Time for Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1603.08983},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.08983},
  timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Graves16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{grefenstette2015learning,
  title = {Learning to Transduce with Unbounded Memory},
  author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
  booktitle = {Advances in Neural Information Processing Systems 28},
  editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages = {1828--1836},
  year = {2015},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf}
}

@incollection{joulin2015inferring,
  title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  author = {Joulin, Armand and Mikolov, Tomas},
  booktitle = {Advances in Neural Information Processing Systems 28},
  editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages = {190--198},
  year = {2015},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf}
}

@article{kaiser2015neural,
  author    = {Lukasz Kaiser and
               Ilya Sutskever},
  title     = {Neural GPUs Learn Algorithms},
  journal   = {CoRR},
  volume    = {abs/1511.08228},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08228},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KaiserS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{kumar2015ask,
  author    = {Ankit Kumar and
               Ozan Irsoy and
               Jonathan Su and
               James Bradbury and
               Robert English and
               Brian Pierce and
               Peter Ondruska and
               Ishaan Gulrajani and
               Richard Socher},
  title     = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1506.07285},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.07285},
  timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KumarISBEPOGS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{kurach2015neural,
  author    = {Karol Kurach and
               Marcin Andrychowicz and
               Ilya Sutskever},
  title     = {Neural Random-Access Machines},
  journal   = {CoRR},
  volume    = {abs/1511.06392},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06392},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KurachAS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{neelakantan2015neural,
  author    = {Arvind Neelakantan and
               Quoc V. Le and
               Ilya Sutskever},
  title     = {Neural Programmer: Inducing Latent Programs with Gradient Descent},
  journal   = {CoRR},
  volume    = {abs/1511.04834},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04834},
  timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/NeelakantanLS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{olah2015understanding,
  title={Understanding LSTM Networks},
  author={Olah, Christopher},
  url={http://colah.github.io/posts/2015-08-Understanding-LSTMs},
  year={2015}
}

@article{reed2015neural,
  author    = {Scott E. Reed and
               Nando de Freitas},
  title     = {Neural Programmer-Interpreters},
  journal   = {CoRR},
  volume    = {abs/1511.06279},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06279},
  timestamp = {Fri, 08 Apr 2016 07:34:31 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ReedF15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{silver2016mastering,
  doi = {10.1038/nature16961},
  url = {http://dx.doi.org/10.1038/nature16961},
  year = 2016,
  month = {jan},
  publisher = {Nature Publishing Group},
  volume = {529},
  number = {7587},
  pages = {484--489},
  author = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  title = {Mastering the game of Go with deep neural networks and tree search},
  journal = {Nature}
}

@article{vinyals2015neural,
  author    = {Oriol Vinyals and
               Quoc V. Le},
  title     = {A Neural Conversational Model},
  journal   = {CoRR},
  volume    = {abs/1506.05869},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.05869},
  timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/VinyalsL15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{weston2014memory,
  author    = {Jason Weston and
               Sumit Chopra and
               Antoine Bordes},
  title     = {Memory Networks},
  journal   = {CoRR},
  volume    = {abs/1410.3916},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.3916},
  timestamp = {Sun, 02 Nov 2014 11:25:59 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1502.03044},
  volume={2},
  number={3},
  pages={5},
  year={2015},
  publisher={CoRR}
}

@article{zaremba2015reinforcement,
  author    = {Wojciech Zaremba and
               Ilya Sutskever},
  title     = {Reinforcement Learning Neural Turing Machines},
  journal   = {CoRR},
  volume    = {abs/1505.00521},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.00521},
  timestamp = {Mon, 01 Jun 2015 14:13:54 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZarembaS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2773--2781},
  year={2015}
}

@inproceedings{vinyals2015pointer,
  title={Pointer networks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2692--2700},
  year={2015}
}

</script>
